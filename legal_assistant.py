# ==============================================================================
# ASSISTANT JURIDIQUE IA - SCRIPT FINAL
# ==============================================================================

import os
import requests
import json
import io
import time
from PyPDF2 import PdfReader
import google.generativeai as genai
from dotenv import load_dotenv
from firecrawl import FirecrawlApp
from fastapi import FastAPI
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
import numpy as np

# --- CONFIGURATION & GLOBALS ---
load_dotenv()

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
FIRECRAWL_API_KEY = os.getenv("FIRECRAWL_API_KEY")
SERPER_API_KEY = os.getenv("SERPER_API_KEY")

genai.configure(api_key=GEMINI_API_KEY)

GOVERNMENT_SITES_DATABASE = {
    "France": [".gouv.fr", "service-public.fr", "legifrance.gouv.fr"],
    "Gabon": [".gouv.ga", "dgi.ga", "pme.gouv.ga", "anpigabon.com"],
    "USA": [".gov"],
    "UK": [".gov.uk"],
    "Canada": [".gc.ca", ".ca/en/government"],
    "Cameroun": [".cm", "impots.cm"],
}

# Cache en m√©moire simple pour √©viter les requ√™tes r√©p√©t√©es et co√ªteuses
api_cache = {}

# --- MOD√àLES DE DONN√âES Pydantic (contrat de l'API) ---
class QueryRequest(BaseModel):
    question: str
    country: str

class AnswerResponse(BaseModel):
    answer: str
    source_url: str | None

# --- INITIALISATION DE L'APPLICATION FastAPI ---
app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# --- FONCTIONS LOGIQUES DE L'ASSISTANT ---

def get_contextual_query(question: str, country: str) -> str:
    print(f"üåç √âtape 1/5 : Adaptation de la requ√™te pour le pays : {country}...")
    try:
        model = genai.GenerativeModel('gemini-1.5-flash')
        prompt = f"""Tu es un expert en droit compar√©. Traduis le concept de la question "{question}" dans le jargon juridique et fiscal le plus probable pour le pays '{country}' afin d'optimiser une recherche web. Ne retourne QUE la requ√™te de recherche, sans aucune autre explication."""
        response = model.generate_content(prompt)
        optimized_query = response.text.strip().replace('"', '')
        print(f"   -> Requ√™te optimis√©e : {optimized_query}")
        return optimized_query
    except Exception as e:
        print(f"   -> Erreur lors de l'optimisation : {e}")
        return question

def search_for_official_sites(question: str, country: str) -> str | None:
    print(f"üîé √âtape 2/5 : Recherche d'un site officiel pour : '{question}'...")
    url = "https://google.serper.dev/search"
    payload = json.dumps({"q": question, "num": 5})
    headers = {'X-API-KEY': SERPER_API_KEY, 'Content-Type': 'application/json'}
    try:
        response = requests.post(url, headers=headers, data=payload, timeout=10)
        response.raise_for_status()
        results = response.json().get('organic', [])
        if not results:
            print("   -> ‚ùå Aucun r√©sultat organique renvoy√© par Serper.")
            return None
        
        official_keywords = GOVERNMENT_SITES_DATABASE.get(country, ['.gov', '.gouv', 'go.'])
        unwanted_keywords = ['facebook.com', 'youtube.com', 'twitter.com', 'linkedin.com', 'wikipedia.org']
        
        print("   -> üî¨ Analyse des r√©sultats pour trouver une source fiable...")
        for result in results:
            link, title = result['link'], result['title'].lower()
            if any(unwanted in link for unwanted in unwanted_keywords):
                continue
            if any(official in link for official in official_keywords) or any(official in title for official in official_keywords):
                print(f"   -> ‚úÖ Source officielle identifi√©e : {link}")
                return link
        
        print("   -> ‚ö†Ô∏è Aucun site clairement officiel trouv√©. Prise du premier r√©sultat non-ind√©sirable.")
        for result in results:
            if not any(unwanted in result['link'] for unwanted in unwanted_keywords):
                print(f"   -> ‚úÖ Pris par d√©faut (meilleur effort) : {result['link']}")
                return result['link']
        
        return None
    except requests.exceptions.RequestException as e:
        print(f"   -> Erreur lors de la recherche : {e}")
        return None

def scrape_content(source_url: str) -> str | None:
    print(f"üìÑ √âtape 3/5 : Extraction du contenu de la source...")
    if not source_url: return None
    try:
        if source_url.lower().endswith('.pdf'):
            print(f"   -> D√©tection d'un PDF. Utilisation du scraper de PDF...")
            response = requests.get(source_url, timeout=30)
            response.raise_for_status()
            pdf_file = io.BytesIO(response.content)
            reader = PdfReader(pdf_file)
            content = "".join(page.extract_text() for page in reader.pages if page.extract_text())
        else:
            print(f"   -> D√©tection d'une page web. Utilisation de Firecrawl...")
            app_firecrawl = FirecrawlApp(api_key=FIRECRAWL_API_KEY)
            scraped_data = app_firecrawl.scrape_url(source_url)
            content = scraped_data.markdown

        if content:
            print("   -> ‚úÖ Contenu extrait avec succ√®s.")
            return content
        else:
            print("   -> ‚ùå Le scraping n'a retourn√© aucun contenu.")
            return None
    except Exception as e:
        print(f"   -> Erreur lors du scraping : {e}")
        return None

def create_and_search_vector_store(text_content: str, question: str) -> str | None:
    """
    La fonction "Super-Chercheur" (RAG). Elle transforme le document en une base de donn√©es
    vectorielle en m√©moire, puis y recherche les passages les plus pertinents.
    """
    print("üß† √âtape 4/5 : Cr√©ation de la carte s√©mantique (Embeddings)...")
    if not text_content: return None
    try:
        # 1. Chunking : D√©couper le texte en morceaux
        chunks = [chunk for chunk in text_content.split('\n\n') if len(chunk.strip()) > 100]
        if not chunks:
            print("   -> ‚ùå Le document n'a pas pu √™tre d√©coup√© en paragraphes significatifs.")
            return None
        print(f"   -> Document d√©coup√© en {len(chunks)} morceaux.")

        # 2. Embedding : Transformer chaque morceau en vecteur
        embedding_model = 'models/text-embedding-004'
        chunk_embeddings = []
        for i in range(0, len(chunks), 100): # Traitement par lots de 100
            batch = chunks[i:i+100]
            response = genai.embed_content(model=embedding_model, content=batch, task_type="RETRIEVAL_DOCUMENT", title="Texte de loi et obligations fiscales")
            chunk_embeddings.extend(response['embedding'])
            print(f"   -> Lot d'embeddings {i//100 + 1} cr√©√©.")
            if len(chunks) > 100: time.sleep(1) # Pause pour respecter les limites de l'API

        print("   -> ‚úÖ Carte s√©mantique cr√©√©e.")

        # 3. Retrieval : Chercher dans la carte
        print("   -> üéØ Recherche des passages les plus pertinents...")
        question_embedding = genai.embed_content(model=embedding_model, content=question, task_type="RETRIEVAL_QUERY")['embedding']
        
        dot_products = np.dot(np.array(chunk_embeddings), question_embedding)
        top_k_indices = np.argsort(dot_products)[-4:][::-1] # Indices des 4 meilleurs passages
        
        relevant_context = "\n---\n".join([chunks[i] for i in top_k_indices])
        print("   -> ‚úÖ Contexte pertinent assembl√©.")
        
        return relevant_context

    except Exception as e:
        print(f"   -> ‚ùå Erreur lors de la recherche vectorielle : {e}")
        return None

def generate_answer_with_gemini(context: str, question: str, country: str) -> str:
    print(f"‚úçÔ∏è  √âtape 5/5 : G√©n√©ration de la r√©ponse finale...")
    if not context: return "La source officielle a √©t√© analys√©e, mais aucun passage pertinent n'a pu √™tre identifi√© pour r√©pondre √† cette question. Le document ne traite peut-√™tre pas de ce sujet sp√©cifique."
    try:
        model = genai.GenerativeModel('gemini-1.5-flash')
        prompt = f"""Tu es un assistant juridique IA. Ta mission est de r√©pondre pr√©cis√©ment √† la question de l'utilisateur en te basant EXCLUSIVEMENT sur le contexte fourni ci-dessous. Contexte : --- {context} ---. Question : "{question}". Pays concern√© : {country}. Formule une r√©ponse claire, structur√©e et professionnelle. Si le contexte ne permet pas de r√©pondre, indique-le clairement."""
        response = model.generate_content(prompt)
        print("   -> ‚úÖ R√©ponse finale g√©n√©r√©e.")
        return response.text
    except Exception as e:
        print(f"   -> Erreur lors de la g√©n√©ration de la r√©ponse finale : {e}")
        return "Une erreur est survenue lors de la g√©n√©ration de la r√©ponse finale."


# --- POINT D'ENTR√âE PRINCIPAL DE L'API ---
@app.post("/process_query", response_model=AnswerResponse)
async def get_legal_answer_endpoint(request: QueryRequest):
    user_question = request.question
    user_country = request.country
    cache_key = f"{user_country.lower()}:{user_question.lower()}"
    
    if cache_key in api_cache:
        print("‚úÖ R√©ponse trouv√©e dans le cache ! Renvoi instantan√©.")
        return api_cache[cache_key]

    print("-" * 50)
    print(f"Requ√™te re√ßue | Pays : {user_country} | Question : {user_question}")
    print("-" * 50)
    
    # √âtape 1 & 2 : Trouver la source
    search_query = get_contextual_query(user_question, user_country)
    source_url = search_for_official_sites(search_query, user_country)

    # √âtape 3 : Scraper le contenu
    scraped_content = scrape_content(source_url)
    if not scraped_content:
        return AnswerResponse(answer="Impossible de r√©cup√©rer le contenu de la source officielle.", source_url=source_url)

    # √âtape 4 (RAG) : Cr√©er la carte s√©mantique et trouver le contexte pertinent
    refined_context = create_and_search_vector_store(scraped_content, user_question)
    
    # √âtape 5 : G√©n√©rer la r√©ponse finale √† partir de ce contexte de haute qualit√©
    final_answer = generate_answer_with_gemini(refined_context, user_question, user_country)
    
    response_to_send = AnswerResponse(answer=final_answer, source_url=source_url)
    
    print("üíæ Sauvegarde de la r√©ponse dans le cache.")
    api_cache[cache_key] = response_to_send
    
    return response_to_send