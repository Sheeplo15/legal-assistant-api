# ==============================================================================
# ASSISTANT JURIDIQUE IA - VERSION "EXPERT-PRUDENT"
# ==============================================================================
import os
import requests
import json
import io
import time
from PyPDF2 import PdfReader
import google.generativeai as genai
from dotenv import load_dotenv
from firecrawl import FirecrawlApp
from fastapi import FastAPI
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware
import numpy as np

# --- CONFIGURATION & GLOBALS ---
load_dotenv()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
FIRECRAWL_API_KEY = os.getenv("FIRECRAWL_API_KEY")
SERPER_API_KEY = os.getenv("SERPER_API_KEY")
genai.configure(api_key=GEMINI_API_KEY)

GOVERNMENT_SITES_DATABASE = {
    "France": [".gouv.fr"], "Gabon": [".gouv.ga"], "USA": [".gov"], 
    "UK": [".gov.uk"], "Canada": [".gc.ca"], "Cameroun": [".cm", "impots.cm"],
}
api_cache = {}

# --- MOD√àLES DE DONN√âES Pydantic ---
class QueryRequest(BaseModel): question: str; country: str
class AnswerResponse(BaseModel): answer: str; source_url: str | None
class SearchPlan(BaseModel):
    requires_search: bool
    reasoning: str
    search_queries: list[str]
    target_domains: list[str]

# --- INITIALISATION DE L'APPLICATION FastAPI ---
app = FastAPI()
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"])


# --- FONCTIONS LOGIQUES DE L'AGENT ---

def create_search_plan(question: str, country: str) -> SearchPlan:
    print(f"ü§î √âtape 0 : R√©flexion strat√©gique pour '{question}' au '{country}'...")
    try:
        model = genai.GenerativeModel('gemini-1.5-flash')
        prompt = f"""
        Tu es un assistant de recherche juridique expert. Cr√©e un plan de recherche structur√© pour la question: "{question}" pour le pays: "{country}".
        1.  Analyse la question. Est-ce une question simple (ex: capitale) ou complexe n√©cessitant une recherche web?
        2.  Si la recherche n'est pas n√©cessaire, mets "requires_search" √† false.
        3.  Si la recherche est n√©cessaire, cr√©e 2-3 requ√™tes Google optimis√©es avec des termes juridiques locaux.
        4.  Liste les domaines web cibles les plus probables (ex: "anpi-gabon.com", "sante.gouv.ga").
        
        R√©ponds UNIQUEMENT avec un objet JSON au format suivant :
        {{
          "requires_search": boolean,
          "reasoning": "string",
          "search_queries": ["string"],
          "target_domains": ["string"]
        }}
        """
        response = model.generate_content(prompt)
        json_text = response.text.strip().replace("```json", "").replace("```", "")
        plan_data = json.loads(json_text)
        plan = SearchPlan(**plan_data)
        print(f"   -> ‚úÖ Plan de recherche cr√©√©. Requ√™te : '{plan.search_queries[0]}'")
        return plan
        
    except Exception as e:
        print(f"   -> ‚ùå Erreur durant la r√©flexion strat√©gique : {e}. Utilisation d'un plan par d√©faut.")
        return SearchPlan(
            requires_search=True,
            reasoning="Le plan strat√©gique a √©chou√©, utilisation d'une recherche par d√©faut.",
            search_queries=[question],
            target_domains=[]
        )

def search_for_official_sites(queries: list[str], country: str, target_domains: list[str]) -> str | None:
    print(f"üîé √âtape 1 : Ex√©cution du plan de recherche...")
    for query in queries:
        print(f"   -> Tentative avec la requ√™te : '{query}'")
        url = "https://google.serper.dev/search"
        payload = json.dumps({"q": query, "num": 5})
        headers = {'X-API-KEY': SERPER_API_KEY, 'Content-Type': 'application/json'}
        try:
            response = requests.post(url, headers=headers, data=payload, timeout=10)
            response.raise_for_status()
            results = response.json().get('organic', [])
            if not results: continue

            unwanted_keywords = ['facebook.com', 'youtube.com', 'wikipedia.org', 'scribd.com', 'researchgate.net', 'twitter.com', 'linkedin.com']
            official_keywords = GOVERNMENT_SITES_DATABASE.get(country, []) + target_domains
            
            print("   -> üî¨ Analyse stricte des r√©sultats...")
            for result in results:
                link, title = result['link'], result['title'].lower()
                if any(unwanted in link for unwanted in unwanted_keywords):
                    continue
                if any(official in link for official in official_keywords):
                    print(f"   -> ‚úÖ Source officielle identifi√©e : {link}")
                    return link
        except Exception as e:
            print(f"   -> ‚ùå Erreur lors de la tentative avec la requ√™te '{query}': {e}")
            continue
            
    print("   -> ‚ùå Aucune source jug√©e suffisamment fiable n'a √©t√© trouv√©e apr√®s toutes les tentatives.")
    return None

def scrape_content(source_url: str) -> str | None:
    print(f"üìÑ √âtape 2 : Extraction du contenu de la source...")
    if not source_url: return None
    try:
        if source_url.lower().endswith('.pdf'):
            response = requests.get(source_url, timeout=30)
            response.raise_for_status()
            reader = PdfReader(io.BytesIO(response.content))
            return "".join(page.extract_text() for page in reader.pages if page.extract_text())
        else:
            return FirecrawlApp(api_key=FIRECRAWL_API_KEY).scrape_url(source_url).markdown
    except Exception as e:
        print(f"   -> ‚ùå Erreur lors du scraping : {e}")
        return None

def create_and_search_vector_store(text_content: str, question: str) -> str | None:
    print(f"üéØ √âtape 3 : Recherche s√©mantique dans le document...")
    if not text_content: return None
    try:
        chunks = [chunk for chunk in text_content.split('\n\n') if len(chunk.strip()) > 100]
        if not chunks: return None

        embedding_model = 'models/text-embedding-004'
        chunk_embeddings = []
        for i in range(0, len(chunks), 100):
            batch = chunks[i:i+100]
            response = genai.embed_content(model=embedding_model, content=batch, task_type="RETRIEVAL_DOCUMENT")
            chunk_embeddings.extend(response['embedding'])
            if len(chunks) > 100: time.sleep(1.1)

        question_embedding = genai.embed_content(model=embedding_model, content=question, task_type="RETRIEVAL_QUERY")['embedding']
        dot_products = np.dot(np.array(chunk_embeddings), question_embedding)
        top_k_indices = np.argsort(dot_products)[-4:][::-1]
        
        relevant_context = "\n---\n".join([chunks[i] for i in top_k_indices])
        return relevant_context
    except Exception as e:
        print(f"   -> ‚ùå Erreur lors de la recherche vectorielle : {e}")
        return None

def generate_answer_with_gemini(context: str, question: str, country: str) -> str:
    print(f"‚úçÔ∏è  √âtape 4 : G√©n√©ration de la r√©ponse finale...")
    if not context: return "La source officielle a √©t√© analys√©e, mais aucun passage pertinent n'a pu √™tre identifi√© pour r√©pondre √† cette question."
    try:
        model = genai.GenerativeModel('gemini-1.5-flash')
        prompt = f"""Tu es un assistant juridique IA. R√©ponds pr√©cis√©ment √† la question en te basant EXCLUSIVEMENT sur le contexte fourni. Contexte: --- {context} ---. Question: "{question}". Pays: {country}. Formule une r√©ponse claire et structur√©e."""
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        print(f"   -> ‚ùå Erreur lors de la g√©n√©ration finale : {e}")
        return "Une erreur est survenue lors de la g√©n√©ration de la r√©ponse finale."

@app.post("/process_query", response_model=AnswerResponse)
async def get_legal_answer_endpoint(request: QueryRequest):
    user_question = request.question
    user_country = request.country
    cache_key = f"{user_country.lower()}:{user_question.lower()}"
    if cache_key in api_cache:
        print("‚úÖ R√©ponse trouv√©e dans le cache ! Renvoi instantan√©.")
        return api_cache[cache_key]

    print("-" * 50); print(f"Requ√™te re√ßue | Pays : {user_country} | Question : {user_question}"); print("-" * 50)
    
    plan = create_search_plan(user_question, user_country)

    if not plan.requires_search:
        return AnswerResponse(answer=plan.reasoning, source_url=None)

    source_url = search_for_official_sites(plan.search_queries, user_country, plan.target_domains)
    
    if not source_url:
        return AnswerResponse(answer="Impossible de trouver une source officielle fiable pour cette question.", source_url=None)

    scraped_content = scrape_content(source_url)
    if not scraped_content:
        return AnswerResponse(answer="Impossible de r√©cup√©rer le contenu de la source officielle trouv√©e.", source_url=source_url)
    
    refined_context = create_and_search_vector_store(scraped_content, user_question)
    final_answer = generate_answer_with_gemini(refined_context, user_question, user_country)
    
    response_to_send = AnswerResponse(answer=final_answer, source_url=source_url)
    api_cache[cache_key] = response_to_send
    print("üíæ Sauvegarde de la r√©ponse dans le cache.")
    
    return response_to_send